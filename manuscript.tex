% AAMAS 2013 Manuscript

% This file should be compiled with "aamas2013.cls" 
% ----------------------------------------------------------------------
% REMEMBER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
% ----------------------------------------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% STYLE NOTES
%
% TENSE AND VOICE
% * Actual experimental events
% 	We wrote descriptions of actual experimental events using past tense, in 
%	active voice, as the first person plural.
%
% * All other contexts
%	We write all other text using present tense, in active voices, as the
%	the first person plural.  
%
% CONVENTIONS
% flapping-wing, small-scale, lightweight, MAV
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SETUP
\documentclass{aamas2013}

%----------------------------------------------------------------------------%
% PACKAGES
\usepackage{color}
\usepackage[pdftex]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\providecommand{\norm}[1]{\lVert#1\rVert}

%----------------------------------------------------------------------------%
% METADATA
\def \papertitle{Cooperative Control and Modeling for Narrow Passage Traversal with Millirobots}
\def \paperkeywords{AAMAS proceedings, multiagent systems, cooperative control, 
particle filter, micro air vehicle, ground station, visual servoing, pose 
estimation, flapping wing, ornithopter, biomimetic}
\def \paperterms{Algorithms, Performance, Design, Experimentation}
%\def \paperauthors{Ryan C. Julian, Cameron J. Rose, Humphrey Hu, and Ronald S. Fearing}
\def \paperauthors{Paper 580}

% Set title
\title{\papertitle}

% PDF bookmarks
\definecolor{linkCol}{gray}{0.25}
\hypersetup{
    pdftitle={\papertitle},
    pdfauthor={\paperauthors},
    pdfsubject={\paperterms},
    pdfkeywords={\paperkeywords},
    pdfpagemode=UseOutlines, bookmarksopen, bookmarksnumbered,
    colorlinks, linkcolor=linkCol, citecolor=linkCol, urlcolor=linkCol
}

% descriptor list for storyboard figure
\newenvironment{spaced_description}[1]
  {\begin{list}{}%
   {\renewcommand\makelabel[1]{##1:\hfill}%
   \settowidth\labelwidth{\makelabel{#1}}%
   \setlength\leftmargin{\labelwidth}
   \addtolength\leftmargin{\labelsep}}}
  {\end{list}}

%----------------------------------------------------------------------------%
% pdfLatex clean-up 
\pdfpagewidth=8.5truein
\pdfpageheight=11truein

\renewcommand{\topfraction}{0.95}
\renewcommand{\textfraction}{0.05}
\renewcommand{\floatpagefraction}{0.90}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% AUTHORS

%% Paper number instead of authors
%\numberofauthors{1}
%\author{
%\alignauthor Paper  580
%}

\numberofauthors{4}
\author{
\alignauthor Ryan C. Julian\\
       \affaddr{Department of EECS}\\
       \affaddr{Univ. of California, Berkeley}\\
       \affaddr{Berkeley, CA 94720}\\
       \email{ryanjulian@berkeley.edu}
\alignauthor Cameron J. Rose\\
       \affaddr{Department of EECS}\\
       \affaddr{Univ. of California, Berkeley}\\
       \affaddr{Berkeley, CA 94720}\\
       \email{c\_rose@eecs.berkeley.edu}
\alignauthor Humphrey Hu\\
       \affaddr{Robotics Institute}\\
       \affaddr{Carnegie Mellon University}\\
       \affaddr{Pittsburgh, PA 15213}\\
       \email{humhu@cmu.edu}
\and
\alignauthor Ronald S. Fearing\\
       \affaddr{Department of EECS}\\
       \affaddr{Univ. of California, Berkeley}\\
       \affaddr{Berkeley, CA 94720}\\
       \email{ronf@eecs.berkeley.edu}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TITLE
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ABSTRACT
\begin{abstract}
The power, size, and weight constraints of micro air vehicles (MAVs) limit 
their on-board sensing and computational resources. Ground vehicles have 
less mobility than MAVs, but relaxed size constraints, and typically more 
computing power. These specializations present many opportunities for 
robot-robot cooperation. 
In this work, we demonstrate cooperative target-seeking between a 13 gram 
ornithopter MAV and a lightweight ground station using computer vision. We 
develop models for the ornithopter, ground station, and cooperative system 
dynamics. We determine model parameters of the real systems through experimental 
system identification. Finally, we verify those models using experiments on 
narrow passage traversal, and arrive at a cooperative system model which 
accurately predicts the backwards-reachable region for successfully 
negotiating ornithopther flight through narrow passages.

We also introduce a new ornithopter MAV, the 13 gram H$^2$Bird. It 
features clap and fling wings, improves upon previous designs by 
utilizing a carbon fiber airframe, tail rotor, and elevator, and carries a 
2.8 gram payload. We augment the ornithopter's built-in gyroscope-based 
control with a lightweight ground station, which has power and weight 
requirements appropriate for deployment on a ground vehicles with 10 gram 
payloads. The ground station provides heading estimates to the ornithopter by 
running a real-time motion tracking algorithm over a live video stream.
\end{abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ACM METADATA

% Note that the category section should be completed after reference to 
% the ACM Computing Classification Scheme available at
% http://www.acm.org/about/class/1998/.
% A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, 
% performance measures]
\category{I.2.9}{Robotics}{Autonomous Vehicles}
\category{I.2.9}{Robotics}{Distributed Artificial Intelligence}[Multiagent Systems]
\category{I.2.9}{Robotics}{Vision and Scene Understanding}[Motion]

% General terms should be selected from the following 16 terms: 
% Algorithms, Management, Measurement, Documentation, Performance, 
% Design, Economics, Reliability, Experimentation, Security, 
% Human Factors, Standardization, Languages, Theory, Legal Aspects, 
% Verification.
\terms{\paperterms}

% Keywords are your own choice of terms you would like the paper to be indexed 
% by.
\keywords{\paperkeywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\begin{figure}[!tb]
\centering
\includegraphics[width=0.95\linewidth]{figures/h2bird.png}
\caption{H$^2$Bird ornithopter MAV}
\label{fig:h2bird}
\end{figure}
Power, size, and weight constraints on 10 gram scale micro air vehicles (MAV) 
significantly limit their sensing and computational capabilities. MAVs can 
execute only low-complexity control and state estimation algorithms in 
real-time.  The constraints on flapping-wing MAVs are even more restrictive. 
Though they demonstrate safety and noise profiles superior to those of 
rotorcraft MAVs, this comes at the expense of reduced payload.

The non-linear dynamics of flapping-wing flight compound 
these constraints~\cite{cheng:tandrdamping}\cite{humbert:dipteran2}. Much 
past research on flapping-wing MAVs focuses on overcoming the difficulties 
of controlling and modeling flapping-wing vehicles in the face of this
non-linearity~\cite{floreano:flying}. Common approaches to simplify these 
dynamic models include averaging over the wing-beat 
period~\cite{cheng:tandrdamping}\cite{Schenato:flightcontrol}, and using 
linear, low-dimensional models to predict flight 
behavior~\cite{Shigeoka:ornithopter}. These advances led to the development 
of high-performance platforms such as the sub-15 gram DelFly, described by 
deCroon et al.~\cite{delfly:design}. 

Many control laws researchers have developed from dynamic models of 
flapping-wing flight, such as periodic forcing and other control methods 
described by~\cite{doman:dynamics}\cite{khan:longitudinal_control}\cite{leonard:averaging}, 
require powerful on-board computing. In this work, we develop a simpler, 
less computationally-intensive method of control for negotiating narrow 
passages, by exploiting cooperation between specialized robotic agents.

In contrast to flight control, there is considerably less previous work on 
guidance and navigation of flapping-wing MAVs. Baek demonstrated altitude 
control with an external camera~\cite{baek:altitude}. de Croon et al. 
demonstrated obstacle avoidance with an onboard camera and offboard 
processing~\cite{delfly:avoid}. Baek et al. provide one of the few examples 
of completely autonomous target seeking for vehicles at the sub-20 gram 
scale~\cite{baek:tracking}.

There also exists a body of work on the guidance and control of fixed- and 
rotary-wing MAV platforms, but many of these methods make extensive use of 
GPS or motion capture systems~\cite{kanade:3dvision}\cite{kingston:timeattitude}. 
Indoors, using GPS to estimate the pose of an MAV is unreliable, and motion 
capture requires extensive and stationary modification of the environment. 

The literature on cooperative control of MAVs is similarly limited to larger
vehicles or analytical explorations. Hyams et al., Jung et al. and Nebot et
al. all demonstrated cooperative visual servoing for heterogeneous
UGVs~\cite{Hyams1999Cooperative}\cite{Jung1998Range}\cite{Nebot2003Agents}.
Luo, et al. and Mehta, et al. analyze UAV-UGV cooperation from a modeling
perspective~\cite{Luo2011Air}\cite{Mehta2006Adaptive}. Rudol et al.
demonstrated a similar cooperative visual servoing system with UGVs and
MAVs~\cite{Rudol2008Micro}, but the vehicles used were orders of magnitude
larger than the lightweight systems we target, and so were able to take
advantage of high-performance processors and specialized cameras. Stirling et
al. describe a method for cooperation amonst quadrotor MAVs to navigate indoor
environment~\cite{SwarmStirling}. Stationary MAVs provide external sensing for
navigation through the environment, and the network can be extended by using
the exploring MAVs for sensing in undiscovered areas. Their implementation
relies solely on local sensing, although our size range requires that the
actual environment sensing be external to the MAV. Additionally, they do not
consider time constraints in their planning, whereas our MAV has a minimum
forward speed and turning radius.

The small size of lightweight air and ground vehicles necessitates 
specialization, but it also pays dividends in agility. A group of specialized 
heterogeneous, lightweight robots can navigate more challenging terrains and 
interact with their environments in ways that larger, monolithic vehicles 
cannot. Despite this advantage, in order to collectively equal the sensing and 
computational capabilities of larger robots, small robots will also have to interact 
so that multiple robots can benefit from the specialized capabilities of 
cooperating vehicles. In this paper we consider both experimentally, and in 
simulation, the performance of the externally-directed ornithopter to 
demonstrate how millirobotic system can benefit from cooperation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Robotic and Vision Platforms}

We develop a new flapping-winged MAV, the H$^2$Bird, and ground station. We
use these sytems to cooperatively navigate through a narrow passage, by
sensing the position of the MAV and the goal using the ground station.

%----------------------------------------------------------------------------%
\subsection{H$^2$Bird Ornithopter}

\begin{figure}[!tb]
\centering
\includegraphics[height=150pt]{figures/h2bird_axes.pdf}
\caption{H$^2$Bird ornithopter with attitude control axes and labeled 
control surfaces.}
\label{fig:h2Bird_axes}
\end{figure}

To explore these cooperative control concepts, we designed a new 
flapping-wing MAV, known as the H$^2$Bird (Figure~\ref{fig:h2bird}). Built 
around the Silverlit i-Bird RC flyer power train and clap-fling wings\footnote{\raggedright Silverlit Toys Manufactory Ltd.: i-Bird RC Flyer
\href{http://www.silverlit-flyingclub.com/wingsmaster/}
     {http://www.silverlit-flyingclub.com/wingsmaster/}}, 
the H$^2$Bird has a wingspan of 26.5 cm and a flight weight of 13 grams. A 
tail propeller and servo-controlled tail elevator provide control in the yaw 
and pitch axes (Figure~\ref{fig:h2Bird_axes}). The on-board ImageProc 2.4\footnote{ImageProc 2.4: \\
\href{https://github.com/biomimetics/imageproc\_pcb}
     {https://github.com/biomimetics/imageproc\_pcb}} 
controller includes of a 40 MIPS microprocessor, 6 DOF IMU, 
IEEE 802.15.4 radio, VGA camera, and motor drivers, all powered by a 90 mAh 
lithium polymer battery. In routine flight, the H$^2$Bird averages 1.2 m/s 
ground speed, and operates for approximately 10 minutes.

%----------------------------------------------------------------------------%
\subsection{Ground Station Computer Vision Platform}
Cooperative behavior is a promising strategy for autonomous navigation in 
unexplored and unstructured environments. However, to operate in these 
environments, a cooperative system must be fully mobile, not tied to a 
static ground station or motion capture laboratory. To demonstrate the 
feasibility of this approach, we designed our ground station to conform to 
the power and weight constraints of highly mobile millirobotic ground 
vehicles, such as OctoRoACH, which can traverse rough terrains with speed and 
agility~\cite{Pullin2012Dynamic}. 

Rather than power-intensive PC processors, we used the ARM-based BeagleBoard 
single-board computer as a reference computation platform. The BeagleBoard 
consumes approximately one watt while running our computer vision and 
control algorithms. It uses the same processor as popular sub-10 gram 
processor modules--such as the Gumstix Overo and LogicPD Torpedo--which are 
light enough for deployment on lightweight ground vehicles. We pair the 
BeagleBoard with an off-the-shelf consumer USB web camera, for similar image 
quality and resolution to the miniature camera modules available at 
millirobot scale. The BeagleBoard communicates with the H$^2$Bird via a 
USB radio module which implements the IEEE 802.15.4 wireless standard.
%TODO footnotes: Gumstix, LogidPD

Our ground station software is based on the Ubuntu Linux\footnote{\raggedright Ubuntu: 
Canonical Ltd., \href{http://www.ubuntu.com/}{http://www.ubuntu.com/}} distribution
modified with a custom version of the Linux kernel. 
We use the Python\footnote{\raggedright Python: Python Software Foundation, \href{http://www.python.org/}
{http://www.python.org/}} programming language for control, robot 
communication, and telemetry, and the OpenCV\footnote{\raggedright OpenCV: Willow Garage, 
\href{http://opencv.willowgarage.com/wiki/}{http://opencv.willowgarage.com/wiki/}} 
computer vision libraries for image capture and processing.
%TODO mention code is open source, available on GitHub

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{System Concept and Algorithms}

Our system combines both the sensing of the H$^2$Bird and the execution of the
desired headings to steer the robot towards the goal. The sensing is performed
on the ground station using frame differencing and a particle filter. The
H$^2$Bird steers to the heading directed by the ground station using a
quaternion-based proportional-integral-derivative controller.

%----------------------------------------------------------------------------%
\subsection{Ornithopter Attitude Control}

We implement attitude estimation and control of the H$^2$Bird on-board to 
achieve high sample-rate attitude control. We estimate the vehicle pose by
integrating the IMU, and use a proportional-integral-derivative (PID) 
controller to feed these estimates back to the attitude control flight 
surfaces (Figures~\ref{fig:process_flow} and~\ref{fig:block_diagram}).

\begin{figure}[tb]
\centering
\includegraphics[width=\linewidth]{figures/process_flow.pdf}
\caption{Overview of the cooperative control system.}
\label{fig:process_flow}
\end{figure}

The high pitch angles at which flapping-wing MAVs regularly operate 
distinguish them from other MAVs. Baek demonstrated control on a similar 
vehicle using PID and an Euler angle parameterization of orientation~\cite{baek:tracking}. 
However, Euler angle parameterizations of vehicle orientation suffer from 
singularities around high pitch angles. So we instead represent the vehicle 
orientation with quaternions. The quaternion representation allows us to 
represent relative body and world coordinate angular displacements compactly 
with quaternion multiplication~\cite{hamilton1866elements}.

Our implementation of quaternion-based control is similar to 
Knoebe's~\cite{knoebe:quatcontrol}:
Given a reference pose represented in quaternion form $q_r$, we define the 
error as the rotation $q_e$ required to reach the reference pose from the 
estimated pose $q$:
\begin{align}
\label{eq:quat_error}
q_e &= q^{\prime}\cdot q_r \\
		&= q_{e,w} + q_{e,x}\hat{i} + q_{e,y}\hat{j} + q_{e,z}\hat{k} \notag 
\end{align}
We then convert this error quaternion to an angle axis representation and 
use it as input to the PID controller (Equations~\ref{eq:quat_linearize} and~\ref{eq:quat_angle}).
We perform these calculations on-board, with trigonometric lookup tables for 
real-time performance.
\begin{align}
\label{eq:quat_linearize}
Q_e &= \alpha\cdot q_e/\sin(\alpha /2) \\
		&= \alpha_{x}\hat{i} + \alpha_{y}\hat{j} + \alpha_{z}\hat{k} \notag
\end{align}
\begin{equation}
\label{eq:quat_angle}
\alpha = 2\arccos(q_{e,w})
\end{equation}

%----------------------------------------------------------------------------%
\subsection{Ground Station Pose Estimation}
The ground station performs simple two-dimensional pose estimation on 
H$^2$Bird using a particle filter-based motion tracking algorithm~\cite{thrun2005probabilistic}. This pose 
estimate is the feedback input to the cooperative visual servoing feedback 
loop, which is represented as the outermost feedback paths on the block 
diagrams in Figures~\ref{fig:process_flow} and~\ref{fig:block_diagram}. We 
limit our pose estimation approach to monocular two-dimensional object 
tracking in pixel space. This allows us to perform real-time video tracking 
using the modest computational resources available on the ground station, 
and similarly, millirobotic ground vehicles.

We initialize the particles 
uniformly across the frame before tracking begins. To improve numerical 
stability, we normalize the weights of all particles on each iteration, and 
only resample the particle population when the mean particle weight falls 
below a pre-determined threshold. 
%............................................................................%
\subsubsection{Motion Model}
Our motion model (Equation~\ref{eq:motion_model}) is a simple Gaussian transition 
centered around each particle's current position, augmented with an 
$\epsilon$-random uniform sampling strategy~\cite{Doucet2001}. The sampling law is as follows:
\begin{equation}
\label{eq:motion_model}
x^{[i]}_t \sim \begin{cases}
\mathcal{N}(x^{[i]}_{t-1}, \Sigma), &\text{with probability } 1-\epsilon \\
\mathcal{U}(0,b),& \text{with probability } \epsilon \\
\end{cases}\\
\end{equation}
In Equation~\ref{eq:motion_model}, $x^{[i]}_t$ is the position of the $i^\text{th}$ 
particle in 640x480 pixel space at time $t$, $\Sigma$ is a chosen covariance matrix, and $b$ 
is a vector of the bounds of the pixel space. The sampling method requires that, with 
probability $\epsilon$, the particle filter will choose a pixel from a uniform 
distribution of all of the pixels in the pixel space for the location of the $i^\text{th}$ 
particle. The $\epsilon$-random uniform transitions increase tracking robustness, by 
forcing the filter to always sample broadly across the pixel space. This helps prevent 
overconfidence and collapse, and decreases reacquisition delay. 

In the experiments we present in this work, we used a diagonal covariance matrix 
with a variance of 256, and an $\epsilon$ of 0.3. We find the Gaussian 
transition model is sufficient for tracking motion in which frame-to-frame 
position changes are relatively small.

%............................................................................%
\subsubsection{Emission Model}
The emission model weights the particles' likelihood of containing a pixel
location that is part of the H$^2$Bird~\cite{Doucet2001}. The particles with
higher weights have a greater probability of being resampled. Our emission
model implements a sampling approach to motion tracking via na\"{i}ve
background subtraction, e.g. frame differencing~\cite{Ahad2011Computer}:
\begin{equation}
\label{eq:emission_model}
w^{[i]}_t = 1 + \norm{f_0(x^{[i]}_t)-f_t(x^{[i]}_t)}^2
\end{equation} 
On each iteration, the system provides the emission 
model with $f_0$, the frame representing the background, and $f_t$, the most 
recent frame captured. The model assigns a weight $w^{[i]}_t$ to each 
particle $x^{[i]}_t$, proportional to the color distance in RGB space 
between the pixels of $f_0$ and $f_t$ at the location $x^{[i]}_t$.
Hence, particles are more likely to be resampled in regions of 
the image which are dissimilar in color from the background.

During the experiments presented in this work, we assumed that neither the 
bird, nor any other moving object, were visible in the frame on system 
start. We saved the first frame captured by the system, and provided it as 
the background frame $f_0$ for every iteration of the particle filter. This 
choice of $f_0$ resulted in a sampling approach to na\"{i}ve background 
subtraction. 

Na\"{i}ve background subtraction is brittle to changes in a scene over 
time~\cite{Ahad2011Computer}. However, we find it very efficient, and sufficiently 
reliable for the short time spans--less than five seconds--necessary for 
H$^2$Bird to successfully negotiate narrow openings. Calculating $f_0$ 
using a more sophisticated background extraction would retrain the 
underlying motion tracking features of the filter, while providing the 
algorithm with a more robust way of differentiating subject motion from the 
background.

%----------------------------------------------------------------------------%
\subsection{Cooperative Visual Servoing}
\label{sec:visual_servoing_concept}
The ground station guides the H$^2$Bird by setting the
reference attitude of its on-board attitude 
controller (Figures~\ref{fig:process_flow} and~\ref{fig:block_diagram}). Using 
the estimate of 2D vehicle pose from the pose estimation algorithm, the 
ground station calculates an altitude error and horizontal error. The ground 
station then uses a PID control law to calculate a new reference pitch and yaw, 
which it sends to the H$^2$Bird over radio. 
We apply a fixed-lag window to the PID 
controller integral term to prevent wind-up while we launch the ornithopter. 
We tuned the proportional and derivative terms of this outer controller to 
dampen the overshoot and drift caused by the cooperative system latency.
\begin{figure}[tb]
\centering
\includegraphics[width=\linewidth]{figures/block_diagrams.pdf}
\caption{Overall block diagram of the cooperative control system.}
\label{fig:block_diagram}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{System Model}
\label{sec:system_model}

We developed models using system identification concepts to simulate the
behaviors of the H$^2$Bird and the ground station. We use these models to
determine the set of initial conditions that are most likely to result in
successful window traversal.

%----------------------------------------------------------------------------%
\subsection{Ornithopter Attitude Control}
\label{sec:model_attitude}

We divide our model of the vehicle dynamics into two independent models for 
simplicity. We develop one model for yaw motion, and another for translational motion.

We approximate the yaw dynamics of the ornithopter with a one-dimensional model to 
limit complexity, and because it matches our physical implementation. The H$^2$Bird 
uses an on-board PID controller to follow a particular heading sent by the ground 
station, and the yaw dynamics are largely decoupled from the rest of the fight dynamics, 
so our one-dimensional model is a good representation of the yaw motion:
\begin{equation}
\label{eq:plain_model}
\begin{aligned}
& H(s) = \frac{K}{1 + Rs}
& \text{where }H(s) = \frac{\theta(s)}{u(s)}
\end{aligned}
\end{equation}
We choose the reference heading angle as the input, $u(s)$, to the model, and
model the dynamics of vehicle and on-board PID controller as one system, whose
output, $\theta(s)$, is the physical heading angle of the H$^2$Bird. To model the translational position of
the ornithopter, we use a form of Dubin's car model~\cite{lavalle:planning}:
\begin{equation}
\label{eq:dubin_car}
\begin{aligned}
& \dot{x} = u_{s}sin(\theta)\\
& \dot{y} = u_{s}cos(\theta)\\
& \dot{\theta} = u\\
& u \in U=[-\tan{\phi_{max}},\tan{\phi_{max}}]
\end{aligned}
\end{equation} 
In the car model, $\phi_{max}$ is the maximum steering angle and $u_{s}$ is
the forward speed. We modify this model to treat the angular position $\theta$
as an input rather than a state. The overall motion of the system can be
modeled as shown in Equation~\ref{eq:dubin_car}, where $\theta$ is the
time-domain output of Equation~\ref{eq:plain_model}, $\theta(t)$.

%----------------------------------------------------------------------------%
\subsection{Ground Station Pose Estimation}

Our model of the ground station includes models for both the camera and the
pose estimation computation. Both models contain latencies, which we
identified in several locations in the block diagram in
Figure~\ref{fig:block_diagram}. The camera model experiences a delay in series
with the path between the ``Camera'' and ``Ornithopter Dynamics'' blocks. The
pose estimation algorithm experiences latency in the pose estimation block
when updating the particle filter, and during the transmission of the desired
heading from the external PID controller to the internal PID controller on
the robot, via radio.
 
In addition to the latencies in the system, we model the viewing angle of the
camera as a pixel map from one edge of a 63$^{\circ}$ viewing triangle to the
other. The camera has an image width of 640 pixels. We calculate the desired
heading angle in degrees, for a given distance from the camera:
\begin{equation} 
\label{eq:desired_heading} 
\text{Desired heading} = \frac{640\,\text{px}}{2d\tan{\frac{63^{\circ}}{2}}}{\times\frac{63^{\circ}}{640\,\text{px}}\times x}
\end{equation}
where $d$ is the distance between the ornithopter and the camera, and $x$ is 
the horizontal position of the robot relative to the center of the window in 
meters. This relation reflects the fact that the pose estimation algorithm 
does not determine the depth of the robot, which can be problematic for 
control algorithms. For example, if the ornithopter remains at a constant 
horizontal distance from the window, but moves closer and closer to the 
camera, the ground station's directed heading steadily increases, even 
though the robot has not actually moved farther from the window. These 
problems are most severe in areas close to the camera.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments and Results}
\label{sec:performance}
To bound the performance of the cooperative control system, we determine 
the feasible set of initial conditions that result in successful narrow 
passage traversal, using both analytical and empirical methods. We then 
conduct experiments to validate this model.

%----------------------------------------------------------------------------%
\subsection{System Identification}

In this section, we present the numerical results of the experiments we used to 
determine the values of the parameters in our system model. 

%............................................................................%
\subsubsection{Ornithopter Attitude Control}
\label{sec:flight_control}
We determined the turning speed and turning radius of the H$^2$Bird 
experimentally, by measuring the response of the system to a step input of a 
clockwise 90$^{\circ}$ turn. We measured the response of the system by 
recording the attitude estimates calculated by the ornithopter using its 
on-board gyroscope. We then used MATLAB\footnote{The MathWorks, Inc. Matlab:
\href{http://www.mathworks.com/products/matlab/}
     {http://www.mathworks.com/products/matlab/}} 
to fit a simple low-order model to the step response, as discussed in 
Section~\ref{sec:model_attitude}.

\begin{figure}[tb]
\centering
\includegraphics[width=\linewidth]{figures/step_response_total.pdf}
\caption{H$^2$Bird step response}
\label{fig:step_response}
\end{figure}

The first-order model that we fit to the step response of the onrithopter 
yaw motion is as folows:
\begin{equation}
\label{eq:transfer_func}
H(s) = \frac{0.95}{1+0.62s}
\end{equation}
This model represents the tuning dynamics of the ornithopter as well as the
PID controller providing inputs to the actuators. We present the results of
the the step response experiment, along with the response of the fitted model,
in Figure~\ref{fig:step_response}.

We calculated the step response rise time to be 1.4 seconds. The H$^2$Bird flies at an 
average forward velocity of 1.2 m/s during our experiments, so we estimate 
the minimum turning radius of the robot to be 1.07 meters:
\begin{equation}
\label{eq:min_radius}
\begin{aligned}
\text{Minimum radius} & = \frac{360^{\circ}\times\text{Rise time}\times\text{Flight
speed}}{\text{Final angle}\times 2\pi}\\
& = \frac{360^{\circ}\times 1.4s\times 1.2\frac{m}{s}}{90^{\circ}\times2*\pi} = 1.07 m
\end{aligned}
\end{equation}
In the experiments we present in this work, we choose a flight speed lower 
than the maximum speed of the ornithopter, to facilitate more robust 
tracking.

%............................................................................%
\subsubsection{Ground Station Pose Estimation}
To measure the capture latency of the camera, we wrote a simple test
application which draws a large colored rectangle on the ground station's
screen, then uses the camera to detect the changes in the rectangle's color.
By noting the value of the system timer just before the rectangle changes
color and just after the camera detects the change, the application records
the estimate of the video capture pipeline latency. We averaged the result
over several hundred rapidly-executed trials to determine the capture latency
we used to model the system.

To compute the particle filter and controller latency, we again used system 
timers to determine the average time between the start and end of computation.

We measured a camera capture latency of 40 ms, pose estimation computation 
latency of 12.5 ms, and radio transmission latency of 25 ms.
%----------------------------------------------------------------------------%
\subsection{Model Verification}

We validate our system model experimentally by systematically guiding the
H$^2$Bird through the window at various starting initial conditions. We
compare the results of the experiments to the computed backwards reachable set
of feasible initial conditions and a Monte Carlo simulation to determine the
accuracy of our system model.

%............................................................................%
\subsubsection{Simulation}

\begin{figure*}[tb]
\begin{minipage}[t]{0.45\linewidth}
\includegraphics[width=\textwidth]{figures/feasible_set.pdf}
\caption{Plot of backwards reachable set for successful window traversal.}
\label{fig:feasible_set}
\end{minipage}
\hfill
\begin{minipage}[t]{0.47\linewidth}
\centering
\centering
\includegraphics[width=\textwidth]{figures/heat_map.png}
\caption{Probability of success, resulting from a Monte Carlo simulation of
the reachable set model, for all starting locations.}
%\caption{Probability of success for all starting locations. This graphic is 
%a result of a Monte Carlo simulation of the reachable set model, in which the 
%initial heading of the H$^2$Bird is chosen uniformly at random from $(-90^{\circ},90^{\circ})$.}
\label{fig:heat_map}
\end{minipage}
\end{figure*}

We determine the backwards reachable set of initial states for successful
narrow passage traversal both geometrically and using Monte Carlo simulation.
We also simulate the motion of the ornithopter beginning from various position
and angular heading initial conditions. We use our model, as determined
through system identification as described in Section~\ref{sec:system_model},
using the cooperative control method in Figure~\ref{fig:block_diagram}. The
results are presented in Figures~\ref{fig:feasible_set}
and~\ref{fig:heat_map}.

In Figure ~\ref{fig:feasible_set}, 
the blue lines represent the camera viewing triangle, the black line represents the window, and the black 
dot represents the camera. For the initial conditions, we used a uniform
grid of 5 cm increments within the camera viewing region, and an initial
angular heading perpendicular to the window plane. A success, or feasible 
initial condition, consists of a path that intersects the window at some 
point and does not leave the camera viewing triangle, and is indicated in 
green. The geometrically infeasible region is computed using the minimum 
turning radius and the geometry of the camera's field-of-view, and is 
indicated in blue. The region in red is geometrically feasible, but is 
infeasible as a result of the cooperative control simulation dynamics.

We conducted a Monte Carlo simulation to determine the probability of
successful window navigation for a given point within the testing area. Using
this information we can make assumptions about ideal starting positions for
cooperative control. For each point in a 10 cm grid within the camera
viewing triangle, we simulated 40 trials using an initial heading randomly
sampled from a uniform distribution between -90$^{\circ}$ and 90$^{\circ}$,
with the 0$^{\circ}$ heading perpendicular to the window.
Figure~\ref{fig:heat_map} depicts the results of the Monte Carlo simulation.

As shown in the figure, there is a region in the middle where we had success
in all 40 of the trials, which is a subset of the feasible region presented in
Figure~\ref{fig:feasible_set}. Along the edges of the camera viewing triangle,
however, the probability of success decreases.

%............................................................................%
\subsubsection{Experiments}
\label{sec:experiments_verification}

We tested our cooperative visual servoing system with a simple target seeking
experiment: the H$^2$Bird must fly through a specified obstacle, in this 
case, a wooden frame we used to simulate a window.

\begin{figure}[tb]
\centering
\includegraphics[width=0.9\linewidth]{figures/experiment_cartoon.pdf}
\caption{Conceptual sketch of experimental setup.}
\label{fig:experiment_cartoon}
\end{figure}

As the H$^2$Bird cannot detect the target on its own, we used cooperative 
control with the ground station, which provided remote guidance to 
H$^2$Bird to allow it to navigate through the window. We used a motion capture 
system to record ground truth data. We present a sketch of this experimental 
setup in Figure~\ref{fig:experiment_cartoon}.

\begin{figure}[tb]
\centering
\includegraphics[width=\linewidth]{figures/pf_screencap.pdf}
\caption{Frame sequence from video feed used for tracking. Particles 
are shown in \textcolor{red}{red}, the current state estimate as a 
\textcolor{yellow}{yellow} circle, the window bounding box in
\textcolor{blue}{blue}, and the target location as a \textcolor{green}{green} 
circle.}
%circle. 1.~Particles initialized uniformly across the frame; 2.~Human operator 
%dragging bounding box across the window; 3.~H$^2$Bird enters the frame. 
%Particles converge on its location; 4.~Release H$^2$Bird, which h
%begins cooperative autonomous flight toward the target; 5.~Visual servoing 
%controller overshoots the window on the left, and commands a right-turn 
%maneuver to recover; 6.~H$^2$Bird successfully navigates through the window.}
\label{fig:pf_screencap}
\end{figure}

Each trial included the following steps (see Figure~\ref{fig:pf_screencap}):
\begin{spaced_description}{\textbf{(4-6)}}
\item[\textbf{(1,2)}] We identify the window to the ground station by dragging a bounding 
box over it using a mouse. 
\item[\textbf{(3)}] We release the H$^2$Bird by hand, in view of the camera at the desired 
starting grid point.
\item[\textbf{(4-6)}] The H$^2$Bird attempts to fly through the window, with remote 
guidance from the ground station.
\end{spaced_description}

Every 5 trials, we replaced the ornithopter's battery with a fully 
charged one.

We collected 80 trials over a variety of starting positions. Our 
goal for these experiments was to verify the model described in 
Section~\ref{sec:system_model}, so we conducted 60 of the 80 trials in a 
grid along the edges of the camera view plane, and in 5 cm increments 
from the window plane, in an effort to determine the success rate on the edges of 
the testing space. We conducted 20 additional trials directly in front of 
the camera, to determine the success rate near the center of viewing plane.

\begin{figure*}[tb]
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/flight_paths_feasible.pdf}
\caption{Plot of camera field of view and experimental trials within the feasible region.}
\label{fig:flight_paths_feasible}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{figures/flight_paths.pdf}
\caption{Plot of camera field-of-view and perimeter experimental trials overlayed.}
\label{fig:flight_paths}
\end{minipage}
\end{figure*}

As a result of the experimental trials, we achieved a success rate of 
80\% for initial conditions in the middle of the testing space and in front 
of the camera. As we moved towards the edges of the feasible region in 
Figure~\ref{fig:flight_paths}, the success rate diminished to 50\%.

The lower success rate on the edges of the camera frame can be explained by
several factors. On the edges of the viewing triangle, the yaw control inputs
are large in magnitude, although this is not necessary in all cases. Since the
control algorithm we are using has no notion of depth, it applies the same
control input whether the ornithopter is close to the camera, where small
changes in heading are adequate, or far away from the camera, where large
changes in heading are necessary. Due to this phenomenon, the controller often
over or under-compensates in certain regions, depending upon the PID tuning
and the distance from the camera. Tracking noise and variations in the
H$^2$Bird hand launch can cause the system to respond differently for similar
initial conditions, or to fail completely. In addition, we changed the
H$^2$Bird battery approximately every 5 tests, adding more variation to the
H$^2$Bird performance by affecting the motor power output.

The results of our experimental trials verify the results of the Monte Carlo
simulation of our model. The trials that we conducted in
Figure~\ref{fig:flight_paths_feasible} correspond to the circled region in
Figure~\ref{fig:heat_map}. The 80\% success rate that we measured in the
experiments corroborates our calculated probability of success of
approximately 80\%, demonstrated by the Monte Carlo simulation. Additionally,
the diminished success rate of 50\% near the edges of the viewing triangle
that we determined through experimentation supports the reduced probability of
success calculated by the simulation.

Our experimental data validates Figure~\ref{fig:feasible_set}, as there are 
no successes within the geometrically infeasible region. There are few 
successes near the boundary of the infeasible region determined in 
simulation. While there are successes along the boundary of the camera 
viewing triangle, there are also many failures.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions and Future Work}
We have demonstrated cooperative guidance of a 13 gram flapping-wing MAV with
a lightweight base station. In addition, we developed a model of
the cooperative system using system identification methods on the individual
system capabilities, and composing it with known system interactions. The
corroboration between our simulated and experimental results shows that our
simplified modeling approach is useful for predicting the performance of
cooperative systems at low computational cost. Our model could also be used to
determine when to begin control with high probability of success, even though
we lack information about the complete pose of the MAV.

We intend to further explore the benefits and limitations of collaboration in
unstructured environments by deploying our base station on millirobotic ground
vehicles. As the ground station used in this work was designed with this goal
in mind, this is a matter of implementation. The effectiveness of our modeling
approach, when applied to more complex systems with more interactions and 
agents is also a future area of interest. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Acknowledgments}
The authors would like to thank Fernando Garcia Bermudez for his 
assistance with the Vicon motion capture system, Andrew Pullin for his 
help with robot photography, and the members of the Biomimetic 
Millisystems Laboratory and the EECS community at the University of 
California, Berkeley for their advice and support. This material is based upon 
work supported by the U.S. Army Research Laboratory under the Micro Autonomous 
Systems and Technology Collaborative Technology Alliance, and the National 
Science Foundation under Grant No. IIS-0931463.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BIBLIOGRAPHY

% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{manuscript}

% ACM needs 'a single self-contained file'!
%\subsection{References}
% Generated by bibtex from your ~.bib file.  Run latex,
% then bibtex, then latex twice (to resolve references)
% to create the ~.bbl file.  Insert that ~.bbl file into
% the .tex source file and comment out
% the command \texttt{{\char'134}thebibliography}.
\end{document}
