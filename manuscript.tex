% AAMAS 2013 Manuscript
%
% This file should be compiled with "aamas2013.cls" 
% ----------------------------------------------------------------------
% REMEMBER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
% ----------------------------------------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% STYLE NOTES
%
% TENSE AND VOICE
% * Actual experimental events
% 	We wrote descriptions of actual experimental events using past tense, in 
%	active voice, as the first person plural.
%
% * All other contexts
%	We write all other text using present tense, in active voices, as the
%	the first person plural.  
%
% CONVENTIONS
% flapping-wing, small-scale, lightweight, MAV
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SETUP
\documentclass{aamas2013}

%----------------------------------------------------------------------------%
% PACKAGES
\usepackage{color}
\usepackage[pdftex]{hyperref}

%----------------------------------------------------------------------------%
% METADATA
\def \papertitle{Cooperative Control for Narrow Passage Traversal with an 
Ornithopter Micro Air Vehicle and Lightweight Ground Station}
\def \paperkeywords{AAMAS proceedings, multiagent systems, cooperative control, 
particle filter, micro air vehicle, ground station, visual servoing, pose 
estimation, flapping wing, ornithopter, biomimetic}
\def \paperterms{Algorithms, Performance, Design, Experimentation}
\def \paperauthors{Ryan C. Julian, Cameron J. Rose, Humphrey Hu, and Ronald S. Fearing}

% Set title
\title{\papertitle}

% PDF bookmarks
\definecolor{linkCol}{gray}{0.25}
\hypersetup{
    pdftitle={\papertitle},
    pdfauthor={\paperauthors},
    pdfsubject={\paperterms},
    pdfkeywords={\paperkeywords},
    pdfpagemode=UseOutlines, bookmarksopen, bookmarksnumbered,
    colorlinks, linkcolor=linkCol, citecolor=linkCol, urlcolor=linkCol
}

%----------------------------------------------------------------------------%
% pdfLatex clean-up 
\pdfpagewidth=8.5truein
\pdfpageheight=11truein

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% AUTHORS

%% Paper number instead of authors
%\numberofauthors{1}
%\alignauthor Paper  580

\numberofauthors{4}
\author{
\alignauthor Ryan C. Julian\\
       \affaddr{Department of EECS}\\
       \affaddr{Univ. of California, Berkeley}\\
       \affaddr{Berkeley, CA 94720}\\
       \email{ryanjulian@berkeley.edu}
\alignauthor Cameron J. Rose\\
       \affaddr{Department of EECS}\\
       \affaddr{Univ. of California, Berkeley}\\
       \affaddr{Berkeley, CA 94720}\\
       \email{c\_rose@eecs.berkeley.edu}
\alignauthor Humphrey Hu\\
       \affaddr{Robotics Institute}\\
       \affaddr{Carnegie Mellon University}\\
       \affaddr{Pittsburgh, PA 15213}\\
       \email{humhu@cmu.edu}
\and
\alignauthor Ronald S. Fearing\\
       \affaddr{Department of EECS}\\
       \affaddr{Univ. of California, Berkeley}\\
       \affaddr{Berkeley, CA 94720}\\
       \email{ronf@eecs.berkeley.edu}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TITLE
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ABSTRACT
\begin{abstract}
The power, size, and weight constraints of micro air vehicles (MAVs) limit 
their on-board sensing and computational resources. Ground vehicles have 
less mobility than MAVs, but relaxed size constraints and typically more 
computing power, creating opportunities for cooperation. In this work, we 
demonstrate cooperative target-seeking between a 13 gram ornithopter MAV 
and a lightweight ground station using computer vision.

We also introduce a new ornithopter MAV, the 13 gram H$^2$Bird. It 
features clap and fling wings, improves upon a previous design by 
utilizing a carbon fiber airframe, tail rotor, and elevator, and carries a 
payload of 2.8 grams. We augment the ornithopter's built-in 
gyroscope-based control with a ground station, which has power and weight 
requirements appropriate for deployment on a 30 gram ground vehicle. The ground 
station provides heading estimates to the ornithoper by running a 
real-time motion tracking algorithm on a live video stream. This allows 
the ornithopter to reliably negotiate flight through narrow openings.

We demonstrate the performance of the location estimation and navigation 
algorithms. We model the backwards-reachable region for successfully 
navigating through narrow openings. Finally, we verify this model through 
with collected in experiments with window traversal.
\end{abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ACM METADATA

% Note that the category section should be completed after reference to 
% the ACM Computing Classification Scheme available at
% http://www.acm.org/about/class/1998/.
% A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, 
% performance measures]
\category{I.2.9}{Robotics}{Autonomous Vehicles}
\category{I.2.9}{Robotics}{Distributed Artificial Intelligence}[Multiagent Systems]
\category{I.2.9}{Robotics}{Vision and Scene Understanding}[Motion]

% General terms should be selected from the following 16 terms: 
% Algorithms, Management, Measurement, Documentation, Performance, 
% Design, Economics, Reliability, Experimentation, Security, 
% Human Factors, Standardization, Languages, Theory, Legal Aspects, 
% Verification.
\terms{\paperterms}

% Keywords are your own choice of terms you would like the paper to be indexed 
% by.
\keywords{\paperkeywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%TODO(ryan)
The power, size, and weight constraints of micro air vehicle (MAV) 
platforms significantly limit on-board sensing and computational 
resources, and thus the complexity of the control and state estimation 
algorithms which can be executed on-board. These constraints are 
particularly severe for flapping wing MAVs, a class of MAVs that have inherently superior 
safety, efficiency, and noise profile to rotor craft MAVs, but at the 
expense of payload weight.
%Intro portion about previous works in ornithopter cooperative control/
%modeling ( won't necessarily stay in this spot)
Modeling and control of flapping wing MAVs is a well studied subject. 
Shigeoka discusses velocity and altitude control in \cite{Shigeoka:ornithopter}. In 
\cite{delfly:design} deCroon et al. 

Due to restrictive payload weights, much previous work on guidance of flapping wing 
with cameras depends on powerful offboard processing. Baek demonstrated altitude control 
with an external camera\cite{baek:altitude}. de Croon et al. demonstrated obstacle 
avoidance with an onboard camera\cite{delfly:avoid}. [However, these papers have not studied
the system blah blah blah.]

Autonomous control of flapping winged MAVs is a complex undertaking because
of the non-linear flight dynamics and underactuation of the system due to size
constraints. Many of the methods for controlling these platforms make
extensive use of GPS in outdoor environments [CITATIONS] and extensive motion
capture systems in indoor environments. While operating indoors, using GPS to 
estimate the pose of the MAV is not possible, and motion capture requires expensive
and stationary modification of the environment. The power and payload limits on MAVs
make on-board sensing difficult, so multi-agent cooperative control schemes are 
apt solutions.

Many of the control laws that previous researches have developed from the dynamic models of flapping-winged flight are computationally complex and require powerful on-board computing [CITATIONS]. In our work, we develop a simpler, less computationally intensive method of control for negotiating narrow passages.

\begin{figure}[!tb]
\centering
\includegraphics[width=\linewidth]{figures/h2bird.png}
\caption{H$^2$Bird Ornithopter MAV}
\label{fig:h2bird}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Robotic and Vision Platforms}
%TODO ryan

%----------------------------------------------------------------------------%
\subsection{H$^2$Bird Ornithoper MAV}
% humphrey
In order to conduct these experiments on cooperation we designed a new
flapping-wing MAV, known as the H$^2$Bird. Built around the Silverlit I-Bird 
RC flyer power train and clap-fling wings\footnote{\url{http://www.silverlit-flyingclub.com/wingsmaster/}}, 
the H$^2$Bird's wingspan is 26.5 cm and it has a flight weight of 13 grams. A tail propeller and 
servo-controlled tail elevator provide control in the yaw and pitch axes. The 
on-board ImageProc 2.4 electronics package consists of a 40 MIPS microprocessor, 
6 DOF IMU, IEEE 802.15.4 radio, VGA camera, and motor drivers, all powered by 
a 90 mAh lithium polymer battery\footnote{Expand! Accredit it to Stan Baek!}. In routine flight, the H$^2$Bird 
averages 1.2 m/s ground speed and can operate for approximately 10 minutes.
%TODO footnote for Silverlit I-Bird
%TODO footnote for ImageProc

%----------------------------------------------------------------------------%
\subsection{Ground Station Computer Vision Platform}
%TODO ryan

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{System Concept and Algorithms}
%TODO ryan

\begin{figure}[tb]
\centering
\includegraphics[width=\linewidth]{figures/process_flow.pdf}
\caption{Overview of the cooperative control system.}
\label{fig:process_flow}
\end{figure}

%----------------------------------------------------------------------------%
\subsection{Ornithoper Flight Control}

\begin{figure}[tb]
\centering
\includegraphics[height=150pt]{figures/h2bird_axes.pdf}
\caption{H$^2$Bird Ornithopter with flight control axes}
\label{fig:h2Bird_axes}
\end{figure}
%TODO check ibird labels

Attitude estimation and control for the H$^2$Bird runs on-board to achieve 
acceptable tracking performance. We estimate the vehicle pose by naively
integrating the IMU, and use PID for controlling the vehicle.
%TODO Cite Stan paper
One behavior of the H$^2$Bird that separates it from other MAVs is the
high pitch angles achieved in routine flight. Baek\cite{baek:tracking} 
demonstrated control on a similar vehicle using PID and an Euler angle
parameterization of orientation. However, Euler angles suffer from singularities
around high pitch angles, so we instead represent the vehicle orientation
with quaternions. An additional benefit of the quaternion representation is 
the ability to represent relative body and world coordinate angular displacements 
compactly with quaternion multiplication

Our implementation of quaternion-based control is similar to Knoebe's\cite{knoebe:quatcontrol}: 
Given a reference quaternion $q$, we define the error $q_e$ as the rotation 
required to reach the reference pose from the measured pose. We then 
convert this error quaternion to an angle axis representation which is given
as an input to the PID controller. 

\begin{equation}
\label{quat_error}
q_e = q'\otimes q_r
\end{equation}
\begin{equation}
\label{quat_angle}
\alpha = 2acos(q_{e,w})
\end{equation}
\begin{equation}
\label{quat_linearize}
Q_e = \alpha*q_e/sin(\alpha /2)
\end{equation}

%----------------------------------------------------------------------------%
\subsection{Pose Estimation}
%TODO ryan

%----------------------------------------------------------------------------%
\subsection{Visual Servoing}
%TODO ryan

%TODO trim PDF
\begin{figure}[tb]
\centering
\includegraphics[width=\linewidth]{figures/block_diagrams.pdf}
\caption{Overall block diagram of cooperative control}
\label{fig:block_diagram}
\end{figure}

\begin{figure}[tb]
\centering
\includegraphics[width=\linewidth]{figures/pf_screencap.pdf}
\caption{Frame sequence from video feed used for tracking. Particles 
are shown in \textcolor{red}{red}, the current state estimate as a 
\textcolor{blue}{blue} circle, the window bounding box in
\textcolor{red}{red}, and the target location as a \textcolor{green}{green} 
circle. 1.~Particles initialized uniformly across the frame; 2.~Human operator 
dragging bounding box across the window; 3.~H$^2$Bird enters the frame. 
Particles converge to its location immediately; 4.~Release H$^2$Bird, which 
begins cooperative autonomous flight toward the target; 5.~Visual servoing 
controller overshoots the window on the left, and commands H$^2$Bird with a 
right-turn maneuver to recover; 6.~H$^2$Bird successfully navigates through 
the window.}
\label{fig:pf_screencap}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
%TODO ryan
The task we chose to test our cooperative visual servoing system with is a 
simple target seeking one: the H$^2$Bird must fly through a specified obstacle,
in this case, a wooden frame meant to simulate a window. As the H$^2$Bird does
cannot detect the target on its own, it depends on remote guidance from the
base station. Ground truth data was collected via a motion capture system in
the test room. A sketch of the experimental setup is shown below in Figure 
~\ref{fig:experiment_cartoon}.

% TODO clarify or roll into paragraph
Trials were run as follows:
\begin{enumerate}
\item The window is identified by a user. 
\item The H$^2$Bird is released in view of the camera by hand.
\item The H$^2$Bird attempts to fly to the window with remote guidance from
the base station.
\end{enumerate}

% TODO Fill in test # per battery
In total, [50+] good trials were run over a variety of starting positions. 
Variations in the H$^2$Bird hand launch caused the system to respond very 
differently for similar initial conditions. In addition, the H$^2$Bird 
battery was changed approximately every [10] tests, adding more variation to
the H$^2$Bird performance by affecting the motor power output. The effects of 
these noisy factors is considered in Section ~\ref{sec:performance}.

\begin{figure}[tb]
\centering
\includegraphics[width=\linewidth]{figures/experiment_cartoon.pdf}
\caption{Conceptual sketch of experimental environment. Dimensions are in meters.}
\label{fig:experiment_cartoon}
\end{figure}
%TODO label experiment cartoon

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Performance}
\label{sec:performance}
%TODO cameron
To bound the performance of our system described in the previous sections, we
determined the feasible set of initial conditions that resulted in successful
window traversal. We then conducted experiments to calculate the success rate
both within the feasible region and on the edges of the feasible region. The
following sections detail how we calculated the feasible and infeasible
regions using system identification and present the results of our
experimental trials.

%----------------------------------------------------------------------------%
\subsection{Ornithopter Flight Control}
\label{sec:flight_control}
%TODO cameron
\begin{figure}[tb]
\centering
\includegraphics[width=\linewidth]{figures/step_response_total.pdf}
\caption{H$^2$Bird step response}
\label{fig:step_response}
\end{figure}
%TODO reduce number of sig figs

We determined the turning speed and turning radius of the H$^2$Bird
experimentally using the response of the system to the step input of a
clockwise 90 degree turn. We depict the results of this experiment in
Figure~\ref{fig:step_response}. We gathered the data for the experiment by 
integrating the output of H$^2$Bird's on-board gyroscope data. We used MATLAB to
fit a simple low-order model, $H(s) = \frac{0.94782}{1+0.61998s}$, of the
turning behavior to the step response with the physical heading as the state
and the heading set point as the input. This model serves as a simplification
of both the turning dynamics of the ornithopter and the on-board PID
controller regulating the actuator inputs to reach the desired orientation. We
use this model in Section~\ref{sec:visual_servoing} to determine the backwards
reachable region for navigating through the window in simulation.

The step response rise time is approximately 1.4 seconds. The H$^2$Bird
flies at an average forward velocity of 1.2 m/s, so the estimated minimum
turning radius of the robot is 1.07 meters. In experiments, we choose a flight
speed lower than the maximum speed of the ornithopter, to facilitate more
robust tracking.
%TODO footnote for MATLAB

%----------------------------------------------------------------------------%
\subsection{Visual Servoing}
\label{sec:visual_servoing}
%TODO cameron

%TODO make these span the page, with identical scales/style
\begin{figure*}[tb]

\begin{minipage}{\linewidth}
\centering
\includegraphics[width=0.45\textwidth]{figures/feasible_set.pdf}
\caption{Plot of camera field-of-view and backwards reachable set for successful window traversal.}
\label{fig:feasible_set}
\end{minipage}
\vspace{1em}
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{figures/flight_paths_feasible.pdf}
\caption{Plot of camera field of view and experimental trials within the feasible region.}
\label{fig:flight_paths_feasible}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{figures/flight_paths.pdf}
\caption{Plot of camera field-of-view and perimeter experimental trials overlayed.}
\label{fig:flight_paths}
\end{minipage}
\end{figure*}

\begin{figure}[tb]
\centering
\includegraphics[width=\linewidth]{figures/flight_paths_feasible.pdf}
\caption{Plot of camera field of view and experimental trials within the feasible region.}
\label{fig:flight_paths_feasible}
\end{figure}

We determined the backwards reachable set of initial states for successful
window traversal both geometrically and in simulation. The results are shown
in Figure~\ref{fig:feasible_set}. For the initial conditions, we used a
spatial grid of 0.05 meter increments within the camera viewing region and an
initial angular position perpendicular to the window plane. The geometrically
infeasible region is indicated in blue and was computed using the minimum
turning radius and simple geometry. Both the regions in red and blue indicate
the infeasible region determined in simulation. We ran a simulation on a
motion model similar to Dubin's car in ~\cite{lavalle:planning}, with the
angular heading as the input rather than the angular velocity. We used the
turning model, described in Section~\ref{sec:flight_control}, as the input to
the motion model, with a PID controller steering the entire system to the
center of the window as the input to the tuning model. The infeasible region
calculated in simulation is a superset of the geometrically infeasible region.
The simulation is more realistic as the rotational damping during the turn and
the on-board PID controller cause the real system to react to inputs at a
non-constant angular velocity.

To verify the feasible region experimentally, we conducted trials at initial
distances in approximately 0.4 meter increments from the window along the
edges of the camera image frame. We began each trial with an initial heading
perpendicular to the window plane. The results of the experiments are shown in
Figures~\ref{fig:flight_paths} and~\ref{fig:flight_paths_feasible}.

For initial conditions in the middle of the feasible region and in front of
the camera, we achieved a success rate of 79\%. This case is depicted in
Figure~\ref{fig:flight_paths_feasible}. As we moved towards the edges of the
feasible region in Figure~\ref{fig:flight_paths}, the success rate diminishes
because of several factors. On the edges of the camera frame, the yaw control
inputs are large in magnitude, although this is not necessary in all cases.
Since the control algorithm we are using has no notion of depth, it applies
the same control input whether the ornithopter is close to the camera, where
small changes in heading are adequate, or far away from the camera, where
large changes in heading are necessary. Due to this phenomenon, the controller
often over- or under-compensates in certain regions, depending upon the PID
tuning and the distance from the camera. Tracking noise can also cause the
window navigation to fail.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions and Future Work}
%TODO ryan

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Acknowledgements}
The authors would like to thank Fernando Garcia Bermudez for his 
assistance with the Vicon motion capture system, Andrew Pullin for his 
help with robot photography, and the members of the Biomimetic 
Millisystems Laboratory and the EECS community at the University of 
California, Berkeley for their advice and support.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BIBLIOGRAPHY

% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{manuscript}

% ACM needs 'a single self-contained file'!
%\subsection{References}
% Generated by bibtex from your ~.bib file.  Run latex,
% then bibtex, then latex twice (to resolve references)
% to create the ~.bbl file.  Insert that ~.bbl file into
% the .tex source file and comment out
% the command \texttt{{\char'134}thebibliography}.
\end{document}
